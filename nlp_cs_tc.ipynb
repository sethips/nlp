{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_cs_tc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/yza3+Y10yjtxhEfkilXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shraddha-an/nlp/blob/main/nlp_cs_tc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P568CBV8nPxM"
      },
      "source": [
        "# **Case Study: NLP models for Text Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmwRePl_nOB4"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "In this project, I look at 3 different NLP models for classifying questions on Stack Overflow into 3 categories depending on their quality.\n",
        "\n",
        "This Case Study outlines 3 techniques to achieve the task of text classification:\n",
        "\n",
        "1.   Training Word Embedding\n",
        "2.   Pretrained GloVe Word Embeddings\n",
        "3.   Pretrained Word2Vec Embeddings\n",
        "\n",
        "\n",
        "**Dataset**: [Stack Overflow Questions](https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JJfG7f2ok57"
      },
      "source": [
        "## 1) **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TXLaHOnmdBY"
      },
      "source": [
        "# Importing libraries\n",
        "# Data Manipulation/ Handling\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sb, matplotlib.pyplot as plt\n",
        "\n",
        "# NLP libraries\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.utils import simple_preprocess\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7X-jWkho0IT"
      },
      "source": [
        "# Importing training & testing datasets\n",
        "dataset = pd.read_csv('train.csv')[['Body', 'Y']].rename(columns = {'Body': 'question', 'Y': 'category'})\n",
        "ds = pd.read_csv('valid.csv')[['Body', 'Y']].rename(columns = {'Body': 'question', 'Y': 'category'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZPixtSjpU6W"
      },
      "source": [
        "## **2) NLP Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYw0IZIOo70V"
      },
      "source": [
        "# Removing symbols, stopwords, punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "symbols = re.compile(pattern = '[/<>(){}\\[\\]\\|@,;]')\n",
        "tags = ['href', 'http', 'https', 'www']\n",
        "\n",
        "def text_clean(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes unwanted symbols, punctuation and stop words from a given string.\n",
        "    \"\"\"\n",
        "    s = symbols.sub(' ', s)\n",
        "    for i in tags:\n",
        "        s = s.replace(i, ' ')\n",
        "    cleaned_text = ' '.join(word for word in simple_preprocess(s, deacc = True) if not word in stop_words)\n",
        "    return cleaned_text\n",
        "\n",
        "# Applying the function on the questions column\n",
        "dataset.iloc[:, 0] = dataset.iloc[:, 0].apply(text_clean)\n",
        "ds.iloc[:, 0] = ds.iloc[:, 0].apply(text_clean)\n",
        "\n",
        "# Train & Test subsets\n",
        "X_train, y_train = dataset.iloc[:, 0].values, dataset.iloc[:, 1].values.reshape(-1, 1)\n",
        "X_test, y_test = ds.iloc[:, 0].values, ds.iloc[:, 1].values.reshape(-1, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aePnEQ11tokf"
      },
      "source": [
        "## **3) Categorical Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfqV81XPqUJt"
      },
      "source": [
        "# One Hot Encoding the Categories Column\n",
        "from sklearn.preprocessing import OneHotEncoder as ohe\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "ct = ColumnTransformer(transformers = [('one_hot_encoder', ohe(categories = 'auto'), [0])],\n",
        "                       remainder = 'passthrough')\n",
        "\n",
        "y_train = ct.fit_transform(y_train)\n",
        "y_test = ct.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWo1SnLXtzv3"
      },
      "source": [
        "## **4) Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_BOIy-qtuP_"
      },
      "source": [
        "# Vectorizing our text corpus of questions\n",
        "# Setting some paramters\n",
        "vocab_size = 2000\n",
        "sequence_length = 6700\n",
        "\n",
        "# Tokenization with keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tk = Tokenizer(num_words = vocab_size)\n",
        "tk.fit_on_texts(X_train)\n",
        "\n",
        "X_train = tk.texts_to_sequences(X_train)\n",
        "X_test = tk.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding all questions with zeros\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train_seq = pad_sequences(X_train, maxlen = 6700, padding = 'post')\n",
        "X_test_seq = pad_sequences(X_test, maxlen = 6700, padding = 'post')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akQ5PmeYubxT"
      },
      "source": [
        "## **5) Embedding Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOETV7ivujNm"
      },
      "source": [
        "# Building & Training the model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = vocab_size, output_dim = 5 , input_length = sequence_length))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units = 3, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = 'rmsprop',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer = 'adam', metrics = ['accuracy'], loss = 'categorical_crossentropy')\n",
        "\n",
        "history = model.fit(X_train_seq, y_train, epochs = 20, batch_size = 512, verbose = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwj1yehSurIk"
      },
      "source": [
        "## **6) Evaluating Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9yMSuLbuqdH"
      },
      "source": [
        "# Evaluating model performance on test set\n",
        "loss, accuracy = model.evaluate(X_test_seq, y_test, verbose = 1)\n",
        "print(\"\\nAccuracy: {}\\nLoss: {}\".format(accuracy, loss))\n",
        "\n",
        "# Save the model\n",
        "#model.save('mdl_80.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nPDD_MTu091"
      },
      "source": [
        "# Plotting Accuracy & Loss over epochs\n",
        "sb.set_style('darkgrid')\n",
        "\n",
        "# 1) Accuracy\n",
        "plt.plot(history.history['accuracy'], label = 'training', color = '#5499C7')\n",
        "plt.legend(shadow = True, loc = 'lower right')\n",
        "plt.title('Accuracy Plot over Epochs')\n",
        "plt.show()\n",
        "\n",
        "# 1) Loss\n",
        "plt.plot(history.history['loss'], label = 'training loss', color = 'purple')\n",
        "plt.legend(shadow = True, loc = 'upper right')\n",
        "plt.title('Loss Plot over Epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}